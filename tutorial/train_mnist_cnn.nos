# Train CNN on MNIST digit classification (Nostos + Candle).
#
# Architecture:
#   [1, 28, 28] -> Conv(32, 3x3) -> BN -> ReLU -> MaxPool(2)   [32, 14, 14]
#               -> Conv(64, 3x3) -> BN -> ReLU -> MaxPool(2)   [64, 7, 7]
#               -> Flatten [3136] -> Linear(128) -> ReLU -> Linear(10)
#
# Run: nostos --use candle tutorial/train_mnist_cnn.nos
use candle.*

# Forward pass through CNN.
# training=True uses batch stats for BN, training=False uses running stats.
cnnForward(conv1: Conv2D, bn1: BatchNorm2D, conv2: Conv2D, bn2: BatchNorm2D, fc1: Linear, fc2: Linear, images: Tensor, training: Bool) -> Tensor = {
    x = unsqueeze(images, 1)
    x1 = maxPool2d(relu(bn1.forward(conv1.forward(x), training)), 2)
    x2 = maxPool2d(relu(bn2.forward(conv2.forward(x1), training)), 2)
    h = relu(fc1.forward(x2.flattenFrom(1)))
    fc2.forward(h)
}

main() = {
    println("Loading MNIST from safetensors...")
    data = loadSafetensors("tutorial/data/mnist.safetensors")
    trainImages = getTensor(data, "train_images")
    trainLabels = getTensor(data, "train_labels")
    testImages = getTensor(data, "test_images")
    testLabels = getTensor(data, "test_labels")

    println("  Train: " ++ show(tensorShape(trainImages)))
    println("  Test:  " ++ show(tensorShape(testImages)))

    # Create trainable CNN (Kaiming-initialized)
    params = paramMapCreate()
    conv1 = convLayer(params, 1, 32, 3, 1)     # 1->32 channels, 3x3, pad=1
    bn1 = batchNormLayer(params, 32)
    conv2 = convLayer(params, 32, 64, 3, 1)    # 32->64 channels, 3x3, pad=1
    bn2 = batchNormLayer(params, 64)
    fc1 = linearLayer(params, 3136, 128)        # flatten -> 128
    fc2 = linearLayer(params, 128, 10)          # 128 -> 10 classes

    opt = adam(params, 0.001)

    batchSize = 64
    numEpochs = 5

    println("")
    println("Training CNN on MNIST:")
    println("  Model: Conv(32,3x3) -> BN -> Conv(64,3x3) -> BN -> FC(128) -> FC(10)")
    println("  Optimizer: Adam(lr=0.001)")
    println("  Batch size: " ++ show(batchSize) ++ ", Epochs: " ++ show(numEpochs))
    println("")

    var epoch = 0
    while epoch < numEpochs {
        avgLoss = eachBatch(trainImages, trainLabels, batchSize, (images, labels) => {
            logits = cnnForward(conv1, bn1, conv2, bn2, fc1, fc2, images, true)
            loss = crossEntropyLoss(logits, labels)
            trainStep(opt, loss)
        })
        println("  Epoch " ++ show(epoch + 1) ++ "/" ++ show(numEpochs) ++ " - avg loss: " ++ show(avgLoss))
        epoch = epoch + 1
    }

    # Evaluate on test set (eval mode for batch norm)
    println("")
    println("Evaluating on test set...")
    correct = eachBatchCount(testImages, testLabels, 100, (images, labels) => {
        logits = cnnForward(conv1, bn1, conv2, bn2, fc1, fc2, images, false)
        preds = argmax(logits, 1).squeeze(1)
        countEqual(preds, labels)
    })

    nTest = tensorShape(testImages)[0]
    pct = toFloat(correct) * 100.0 / toFloat(nTest)
    println("  Test accuracy: " ++ show(correct) ++ "/" ++ show(nTest) ++ " = " ++ show(pct) ++ "%")

    0
}
