# Candle - Machine Learning Library for Nostos
#
# This module provides tensor operations using the Candle ML framework.
#
# Usage:
#   use candle.*
#
#   t = zeros([2, 3])
#   t2 = ones([2, 3])
#   result = tensorAdd(t, t2)

# =============================================================================
# Tensor Creation
# =============================================================================

# Create a tensor filled with zeros
pub zeros(shape: List[Int]) -> Tensor = __native__("Candle.zeros", shape)

# Create a tensor filled with ones
pub ones(shape: List[Int]) -> Tensor = __native__("Candle.ones", shape)

# Create a tensor with random normal values (mean=0, std=1)
pub randn(shape: List[Int]) -> Tensor = __native__("Candle.randn", shape)

# Create a tensor from a nested list (float values)
pub fromList(data: List[Float]) -> Tensor = __native__("Candle.fromList", data)

# Create a tensor from integer list (for token IDs)
pub fromIntList(data: List[Int]) -> Tensor = __native__("Candle.fromIntList", data)

# Implicit conversion functions (naming convention: {typeLower}From{SourceType})
# These enable automatic conversion at function call sites:
#   mseLoss(w, [5.0])  =>  mseLoss(w, tensorFromList([5.0]))
pub tensorFromList(data: List[Float]) -> Tensor = __native__("Candle.fromList", data)
pub tensorFromIntList(data: List[Int]) -> Tensor = __native__("Candle.fromIntList", data)

# Create a 1D tensor with values from 0 to n-1
pub arange(n: Int) -> Tensor = __native__("Candle.arange", n)

# =============================================================================
# Binary Operations
# =============================================================================

# Element-wise addition (with broadcasting)
pub tensorAdd(a: Tensor, b: Tensor) -> Tensor = __native__("Candle.add", a, b)

# Element-wise subtraction (with broadcasting)
pub tensorSub(a: Tensor, b: Tensor) -> Tensor = __native__("Candle.sub", a, b)

# Element-wise multiplication (with broadcasting)
pub tensorMul(a: Tensor, b: Tensor) -> Tensor = __native__("Candle.mul", a, b)

# Element-wise division (with broadcasting)
pub tensorDiv(a: Tensor, b: Tensor) -> Tensor = __native__("Candle.div", a, b)

# Matrix multiplication
pub matmul(a: Tensor, b: Tensor) -> Tensor = __native__("Candle.matmul", a, b)

# Element-wise power: tensorPow(t, 2.0) -> t^2
pub tensorPow(t: Tensor, exp: Float) -> Tensor = __native__("Candle.pow", t, exp)

# =============================================================================
# Unary Operations
# =============================================================================

# Element-wise exponential
pub tensorExp(t: Tensor) -> Tensor = __native__("Candle.exp", t)

# Element-wise natural logarithm
pub tensorLog(t: Tensor) -> Tensor = __native__("Candle.log", t)

# Element-wise square root
pub tensorSqrt(t: Tensor) -> Tensor = __native__("Candle.sqrt", t)

# Element-wise tanh
pub tanh(t: Tensor) -> Tensor = __native__("Candle.tanh", t)

# Element-wise negation
pub tensorNeg(t: Tensor) -> Tensor = __native__("Candle.neg", t)

# Element-wise ReLU activation
pub relu(t: Tensor) -> Tensor = __native__("Candle.relu", t)

# Element-wise GELU activation
pub gelu(t: Tensor) -> Tensor = __native__("Candle.gelu", t)

# Element-wise cosine
pub tensorCos(t: Tensor) -> Tensor = __native__("Candle.cos", t)

# Element-wise sine
pub tensorSin(t: Tensor) -> Tensor = __native__("Candle.sin", t)

# Softmax along a dimension
pub softmax(t: Tensor, dim: Int) -> Tensor = __native__("Candle.softmax", t, dim)

# =============================================================================
# Convolution & Pooling (low-level)
# =============================================================================

# 2D convolution: input [batch, channels, H, W], kernel [outC, inC, kH, kW]
#   conv2d(input, kernel, stride, padding)
pub conv2d(input: Tensor, kernel: Tensor, stride: Int, padding: Int) -> Tensor = __native__("Candle.conv2d", input, kernel, stride, padding)

# 2D convolution with bias: adds bias [outChannels] after convolution
#   conv2dBias(input, kernel, bias, stride, padding)
pub conv2dBias(input: Tensor, kernel: Tensor, bias: Tensor, stride: Int, padding: Int) -> Tensor = __native__("Candle.conv2dBias", input, kernel, bias, stride, padding)

# Max pooling 2D: reduces spatial dims by factor of kernelSize
#   maxPool2d(input, 2) on [batch, C, 28, 28] -> [batch, C, 14, 14]
pub maxPool2d(input: Tensor, kernelSize: Int) -> Tensor = __native__("Candle.maxPool2d", input, kernelSize)

# =============================================================================
# Trainable Layers (proper Kaiming initialization)
# =============================================================================
#
# These create layers with correctly initialized weights, registered
# in a parameter map for training. Use these instead of raw paramRandn.
#
#   params = paramMapCreate()
#   conv1 = convLayer(params, 1, 32, 3, 1)    # 1->32 channels, 3x3 kernel, pad=1
#   fc = linearLayer(params, 128, 10)          # 128->10
#
#   x = conv1.forward(input)                   # UFCS forward pass
#   x = fc.forward(flat)

# Create a trainable Conv2D layer with Kaiming-initialized weights and bias.
#   convLayer(params, inChannels, outChannels, kernelSize, padding) -> Conv2D
pub convLayer(params: ParamMap, inChannels: Int, outChannels: Int, kernelSize: Int, padding: Int) -> Conv2D = __native__("Candle.convLayer", params, inChannels, outChannels, kernelSize, padding)

# Forward pass through a Conv2D layer: input [batch, C, H, W] -> output [batch, Cout, Hout, Wout]
pub forward(layer: Conv2D, input: Tensor) -> Tensor = __native__("Candle.convForward", layer, input)

# Create a trainable Linear layer (no bias) with Kaiming-initialized weights.
#   linearLayer(params, inDim, outDim) -> Linear
pub linearLayer(params: ParamMap, inDim: Int, outDim: Int) -> Linear = __native__("Candle.linearLayerAuto", params, inDim, outDim)

# Forward pass through a Linear layer: input [batch, inDim] -> output [batch, outDim]
pub forward(layer: Linear, input: Tensor) -> Tensor = __native__("Candle.linearForward", layer, input)

# Create a BatchNorm2D layer with learnable scale/bias parameters.
#   batchNormLayer(params, numFeatures) -> BatchNorm2D
pub batchNormLayer(params: ParamMap, numFeatures: Int) -> BatchNorm2D = __native__("Candle.batchNormLayer", params, numFeatures)

# Forward pass through BatchNorm2D (training mode — uses batch stats, updates running stats).
#   Input: [batch, channels, H, W], channels must match numFeatures.
pub forward(layer: BatchNorm2D, input: Tensor) -> Tensor = __native__("Candle.bnForward", layer, input)

# Forward pass through BatchNorm2D (eval mode — uses accumulated running stats).
pub forwardEval(layer: BatchNorm2D, input: Tensor) -> Tensor = __native__("Candle.bnForwardEval", layer, input)

# Forward pass with explicit training flag.
#   bn.forward(x, true)  = training mode (batch stats, updates running stats)
#   bn.forward(x, false) = eval mode (running stats)
pub forward(layer: BatchNorm2D, input: Tensor, training: Bool) -> Tensor =
    if training { forward(layer, input) } else { forwardEval(layer, input) }

# =============================================================================
# Shape Operations
# =============================================================================

# Reshape tensor to new shape
pub reshape(t: Tensor, newShape: List[Int]) -> Tensor = __native__("Candle.reshape", t, newShape)

# Flatten all dimensions from fromDim onward into one.
#   [batch, 64, 7, 7].flattenFrom(1) -> [batch, 3136]
#   [64, 7, 7].flattenFrom(0) -> [3136]
pub flattenFrom(t: Tensor, fromDim: Int) -> Tensor = __native__("Candle.flatten", t, fromDim)

# Transpose (swap) two dimensions
pub swapDims(t: Tensor, dim1: Int, dim2: Int) -> Tensor = __native__("Candle.transpose", t, dim1, dim2)

# Remove dimension of size 1
pub squeeze(t: Tensor, dim: Int) -> Tensor = __native__("Candle.squeeze", t, dim)

# Add dimension of size 1
pub unsqueeze(t: Tensor, dim: Int) -> Tensor = __native__("Candle.unsqueeze", t, dim)

# Concatenate tensors along a dimension
pub cat(tensors: List[Tensor], dim: Int) -> Tensor = __native__("Candle.cat", tensors, dim)

# Slice a tensor along a dimension
pub narrow(t: Tensor, dim: Int, start: Int, len: Int) -> Tensor = __native__("Candle.narrow", t, dim, start, len)

# Select indices along a dimension
pub indexSelect(t: Tensor, dim: Int, indices: Tensor) -> Tensor = __native__("Candle.indexSelect", t, dim, indices)

# Make tensor contiguous in memory
pub contiguous(t: Tensor) -> Tensor = __native__("Candle.contiguous", t)

# =============================================================================
# Reductions
# =============================================================================

# Sum all elements
pub tensorSum(t: Tensor) -> Tensor = __native__("Candle.sum", t)

# Sum along a dimension
pub tensorSumDim(t: Tensor, dim: Int) -> Tensor = __native__("Candle.sum", t, dim)

# Mean of all elements
pub tensorMean(t: Tensor) -> Tensor = __native__("Candle.mean", t)

# Mean along a dimension
pub tensorMeanDim(t: Tensor, dim: Int) -> Tensor = __native__("Candle.mean", t, dim)

# Index of maximum value along a dimension
pub argmax(t: Tensor, dim: Int) -> Tensor = __native__("Candle.argmax", t, dim)

# Variance along a dimension
pub tensorVar(t: Tensor, dim: Int) -> Tensor = __native__("Candle.var", t, dim)

# =============================================================================
# Tensor Info & Conversion
# =============================================================================

# Get the shape of a tensor as a list
pub tensorShape(t: Tensor) -> List[Int] = __native__("Candle.shape", t)

# Convert tensor to nested list
pub toList(t: Tensor) -> List[Float] = __native__("Candle.toList", t)

# Clone a tensor
pub tensorClone(t: Tensor) -> Tensor = __native__("Candle.clone", t)

# Get dtype as string ("f32", "f64", "u32", etc.)
pub dtype(t: Tensor) -> String = __native__("Candle.dtype", t)

# =============================================================================
# Neural Network Operations
# =============================================================================

# Layer normalization
pub layerNorm(x: Tensor, gamma: Tensor, beta: Tensor) -> Tensor = __native__("Candle.layerNorm", x, gamma, beta)
pub layerNormEps(x: Tensor, gamma: Tensor, beta: Tensor, eps: Float) -> Tensor = __native__("Candle.layerNorm", x, gamma, beta, eps)

# Embedding lookup
pub embedding(indices: Tensor, embeddings: Tensor) -> Tensor = __native__("Candle.embedding", indices, embeddings)

# Linear layer: x @ weight.T + bias
pub linear(x: Tensor, weight: Tensor) -> Tensor = __native__("Candle.linear", x, weight)
pub linearBias(x: Tensor, weight: Tensor, bias: Tensor) -> Tensor = __native__("Candle.linear", x, weight, bias)

# Dropout (inference mode - no-op)
pub dropout(x: Tensor, p: Float) -> Tensor = __native__("Candle.dropout", x, p)

# =============================================================================
# Model Loading (Safetensors)
# =============================================================================

# Load a safetensors file
pub loadSafetensors(path: String) -> TensorMap = __native__("Candle.loadSafetensors", path)

# Get a tensor from a loaded model by name
pub getTensor(model: TensorMap, name: String) -> Tensor = __native__("Candle.getTensor", model, name)

# List all tensor names in a loaded model
pub listTensors(model: TensorMap) -> List[String] = __native__("Candle.listTensors", model)

# =============================================================================
# Tokenizer
# =============================================================================

# Load a tokenizer from a JSON file (tokenizer.json from HuggingFace)
pub loadTokenizer(path: String) -> Tokenizer = __native__("Candle.loadTokenizer", path)

# Encode text to token IDs
pub encode(tokenizer: Tokenizer, text: String) -> List[Int] = __native__("Candle.encode", tokenizer, text)

# Decode token IDs back to text
pub decode(tokenizer: Tokenizer, ids: List[Int]) -> String = __native__("Candle.decode", tokenizer, ids)

# Get vocabulary size
pub vocabSize(tokenizer: Tokenizer) -> Int = __native__("Candle.vocabSize", tokenizer)

# =============================================================================
# Attention Utilities
# =============================================================================

# Create attention mask from token IDs
pub createAttentionMask(tokenIds: Tensor, padTokenId: Int) -> Tensor = __native__("Candle.createAttentionMask", tokenIds, padTokenId)

# Apply attention mask to attention scores
pub applyAttentionMask(scores: Tensor, mask: Tensor) -> Tensor = __native__("Candle.applyAttentionMask", scores, mask)

# =============================================================================
# ModernBERT Operations
# =============================================================================

# Apply Rotary Position Embeddings (RoPE)
pub applyRope(x: Tensor, cos: Tensor, sin: Tensor) -> Tensor = __native__("Candle.applyRope", x, cos, sin)

# GeGLU activation for MLP
pub geglu(gate: Tensor, up: Tensor) -> Tensor = __native__("Candle.geglu", gate, up)

# RMS Normalization (alternative to LayerNorm)
pub rmsNorm(x: Tensor, weight: Tensor) -> Tensor = __native__("Candle.rmsNorm", x, weight)
pub rmsNormEps(x: Tensor, weight: Tensor, eps: Float) -> Tensor = __native__("Candle.rmsNorm", x, weight, eps)

# Create local attention mask (sliding window)
pub localAttentionMask(seqLen: Int, windowSize: Int) -> Tensor = __native__("Candle.localAttentionMask", seqLen, windowSize)

# Cast tensor to different dtype
pub cast(t: Tensor, dtype: String) -> Tensor = __native__("Candle.cast", t, dtype)

# SiLU activation (Swish): x * sigmoid(x)
pub silu(t: Tensor) -> Tensor = __native__("Candle.silu", t)

# Compute RoPE frequencies
pub ropeFreqs(seqLen: Int, headDim: Int, theta: Float) -> List[Tensor] = __native__("Candle.ropeFreqs", seqLen, headDim, theta)

# =============================================================================
# LSTM
# =============================================================================

# Create LSTM with random weights (Xavier initialization)
pub lstmCreate(inputDim: Int, hiddenDim: Int) -> LSTM = __native__("Candle.lstmCreate", inputDim, hiddenDim)

# Load LSTM weights from a safetensors model
# Prefix selects tensors by name prefix (e.g. "encoder.lstm")
pub lstmFromTensors(model: TensorMap, prefix: String, inputDim: Int, hiddenDim: Int) -> LSTM = __native__("Candle.lstmFromTensors", model, prefix, inputDim, hiddenDim)

# Create zero initial state for given batch size
pub lstmZeroState(lstm: LSTM, batchSize: Int) -> LSTMState = __native__("Candle.lstmZeroState", lstm, batchSize)

# Single LSTM step: input [batch, inputDim] -> [outputTensor, newState]
pub lstmStep(lstm: LSTM, input: Tensor, state: LSTMState) -> List[Tensor] = __native__("Candle.lstmStep", lstm, input, state)

# Full sequence forward pass: input [batch, seq, inputDim] -> hidden states [batch, seq, hiddenDim]
pub lstmSeq(lstm: LSTM, input: Tensor) -> Tensor = __native__("Candle.lstmSeq", lstm, input)

# Full sequence with initial state
pub lstmSeqInit(lstm: LSTM, input: Tensor, state: LSTMState) -> Tensor = __native__("Candle.lstmSeqInit", lstm, input, state)

# Extract hidden tensor from LSTM state
pub lstmHidden(state: LSTMState) -> Tensor = __native__("Candle.lstmHidden", state)

# Extract cell tensor from LSTM state
pub lstmCell(state: LSTMState) -> Tensor = __native__("Candle.lstmCell", state)

# =============================================================================
# Attention
# =============================================================================

# Scaled dot-product attention: softmax(Q * K^T / sqrt(d)) * V
# Q: [..., seqQ, dk], K: [..., seqK, dk], V: [..., seqK, dv] -> [..., seqQ, dv]
pub scaledDotProductAttention(q: Tensor, k: Tensor, v: Tensor) -> Tensor = __native__("Candle.scaledDotProductAttention", q, k, v)

# Multi-head attention with learned projections
# x: [batch, seq, dim], weights: [dim, dim] each
pub multiHeadAttention(x: Tensor, numHeads: Int, wQ: Tensor, wK: Tensor, wV: Tensor, wO: Tensor) -> Tensor = __native__("Candle.multiHeadAttention", x, numHeads, wQ, wK, wV, wO)

# Combined LSTM + multi-head self-attention
# Runs LSTM, then applies attention over hidden states
# input: [batch, seq, inputDim] -> attended output [batch, seq, hiddenDim]
pub lstmAttention(lstm: LSTM, input: Tensor, numHeads: Int, wQ: Tensor, wK: Tensor, wV: Tensor, wO: Tensor) -> Tensor = __native__("Candle.lstmAttention", lstm, input, numHeads, wQ, wK, wV, wO)

# =============================================================================
# Primitives for building layers in Nostos
# =============================================================================

# Sigmoid activation
pub sigmoid(t: Tensor) -> Tensor = __native__("Candle.sigmoid", t)

# Uniform random tensor in [0, 1]
pub randUniform(shape: List[Int]) -> Tensor = __native__("Candle.randUniform", shape)

# Element-wise greater than scalar (returns 0.0/1.0 float mask)
pub gtScalar(t: Tensor, val: Float) -> Tensor = __native__("Candle.gtScalar", t, val)

# =============================================================================
# Dropout Layer (implemented in Nostos)
# =============================================================================

# Training dropout - randomly zeros elements with probability p and scales by 1/(1-p)
# Use this between layers during training:
#   h = lstmSeq(lstm, input)
#   h = dropoutLayer(h, 0.1)
#   out = multiHeadAttention(h, numHeads, wQ, wK, wV, wO)
pub dropoutLayer(x: Tensor, p: Float) -> Tensor = {
    mask = gtScalar(randUniform(tensorShape(x)), p)
    scale = fromList([1.0 / (1.0 - p)])
    tensorMul(tensorMul(x, mask), scale)
}

# =============================================================================
# Training: Parameter Maps
# =============================================================================

# Create an empty parameter map (collects all trainable weights)
pub paramMapCreate() -> ParamMap = __native__("Candle.paramMapCreate")

# Create a trainable tensor with random normal initialization (named, for save/load)
pub paramRandn(params: ParamMap, name: String, shape: List[Int]) -> Tensor = __native__("Candle.paramRandn", params, name, shape)

# Create a trainable tensor with random normal initialization (auto-named)
pub paramRandn(params: ParamMap, shape: List[Int]) -> Tensor = __native__("Candle.paramRandnAuto", params, shape)

# Create a trainable tensor initialized to zeros (named, for save/load)
pub paramZeros(params: ParamMap, name: String, shape: List[Int]) -> Tensor = __native__("Candle.paramZeros", params, name, shape)

# Create a trainable tensor initialized to zeros (auto-named)
pub paramZeros(params: ParamMap, shape: List[Int]) -> Tensor = __native__("Candle.paramZerosAuto", params, shape)

# Save all parameters to a safetensors file
pub paramSave(params: ParamMap, path: String) -> Int = __native__("Candle.paramSave", params, path)

# Load parameters from a safetensors file
pub paramLoad(params: ParamMap, path: String) -> Int = __native__("Candle.paramLoad", params, path)

# =============================================================================
# Training: Trainable LSTM
# =============================================================================

# Create an LSTM with trainable weights registered in the parameter map (named, for save/load).
# The prefix namespaces the weights (e.g. "encoder" -> "encoder.weight_ih_l0").
pub lstmTrainable(params: ParamMap, prefix: String, inputDim: Int, hiddenDim: Int) -> LSTM = __native__("Candle.lstmTrainable", params, prefix, inputDim, hiddenDim)

# Create an LSTM with trainable weights (auto-named)
pub lstmTrainable(params: ParamMap, inputDim: Int, hiddenDim: Int) -> LSTM = __native__("Candle.lstmTrainableAuto", params, inputDim, hiddenDim)

# =============================================================================
# Training: Optimizers
# =============================================================================

# Create SGD optimizer with given learning rate
pub sgd(params: ParamMap, lr: Float) -> Optimizer = __native__("Candle.sgd", params, lr)

# Create Adam optimizer with given learning rate (default betas, eps, weight_decay)
pub adam(params: ParamMap, lr: Float) -> Optimizer = __native__("Candle.adam", params, lr)

# Create Adam optimizer with full control over hyperparameters
pub adamFull(params: ParamMap, lr: Float, beta1: Float, beta2: Float, eps: Float, wd: Float) -> Optimizer = __native__("Candle.adamFull", params, lr, beta1, beta2, eps, wd)

# Change the learning rate of an optimizer
pub setLearningRate(opt: Optimizer, lr: Float) -> Int = __native__("Candle.setLearningRate", opt, lr)

# =============================================================================
# Training: Loss Functions
# =============================================================================

# Mean squared error: mseLoss(predictions, targets) -> scalar loss tensor
pub mseLoss(pred: Tensor, target: Tensor) -> Tensor = __native__("Candle.mseLoss", pred, target)

# Cross-entropy loss: crossEntropyLoss(logits [N,C], targetIds [N]) -> scalar loss tensor
pub crossEntropyLoss(logits: Tensor, targets: Tensor) -> Tensor = __native__("Candle.crossEntropyLoss", logits, targets)

# =============================================================================
# Training: Train Step
# =============================================================================

# Perform one training step: backward pass + optimizer weight update.
# Returns the loss value as a Float.
pub trainStep(opt: Optimizer, loss: Tensor) -> Float = __native__("Candle.trainStep", opt, loss)

# =============================================================================
# Comparison
# =============================================================================

# Count matching elements between two tensors (works with any dtype)
pub countEqual(a: Tensor, b: Tensor) -> Int = __native__("Candle.countEqual", a, b)

# =============================================================================
# Indexing & Slicing
# =============================================================================
#
# Readable alternatives to Python's cryptic [:, -1, :] syntax.
# Only name the dimension you operate on — the rest stay untouched.
#
#   Python                   Nostos
#   hidden[:, -1, :]         hidden.lastOf(1)
#   hidden[:, 0, :]          hidden.firstOf(1)
#   hidden[:, 3, :]          hidden.pick(1, 3)
#   images[100:132, :, :]    images.sliceDim(0, 100, 32)

# Pick a single element along a dimension, removing that dimension.
# Supports negative indices: -1 = last, -2 = second-to-last, etc.
#   t.pick(1, 3)   on [2, 5, 4] -> [2, 4]   (element 3 of dim 1)
#   t.pick(0, -1)  on [2, 5, 4] -> [5, 4]   (last element of dim 0)
pub pick(t: Tensor, dim: Int, idx: Int) -> Tensor = {
    n = tensorShape(t)[dim]
    i = if idx < 0 { n + idx } else { idx }
    squeeze(narrow(t, dim, i, 1), dim)
}

# Pick the first element along a dimension, removing that dimension.
#   hidden.firstOf(1)  on [batch, 28, 128] -> [batch, 128]
pub firstOf(t: Tensor, dim: Int) -> Tensor = pick(t, dim, 0)

# Pick the last element along a dimension, removing that dimension.
#   hidden.lastOf(1)  on [batch, 28, 128] -> [batch, 128]
pub lastOf(t: Tensor, dim: Int) -> Tensor = pick(t, dim, -1)

# Slice a range along a dimension, keeping that dimension.
#   images.sliceDim(0, 100, 32)  on [60000, 28, 28] -> [32, 28, 28]
pub sliceDim(t: Tensor, dim: Int, start: Int, len: Int) -> Tensor = narrow(t, dim, start, len)

# =============================================================================
# Batch Iteration
# =============================================================================

# Iterate over (data, labels) in batches, return average of fn results.
# Use for training loops where fn returns the loss.
#   avgLoss = eachBatch(images, labels, 64, (imgs, lbls) => {
#       logits = model(imgs)
#       loss = crossEntropyLoss(logits, lbls)
#       trainStep(opt, loss)
#   })
pub eachBatch(data: Tensor, labels: Tensor, batchSize: Int, fn: (Tensor, Tensor) -> Float) -> Float = {
    n = tensorShape(data)[0]
    var total = 0.0
    var count = 0
    var start = 0
    while start < n {
        curBatch = min(batchSize, n - start)
        batchData = data.sliceDim(0, start, curBatch)
        batchLabels = labels.sliceDim(0, start, curBatch)
        total = total + fn(batchData, batchLabels)
        count = count + 1
        start = start + batchSize
    }
    total / toFloat(count)
}

# Iterate over (data, labels) in batches, return sum of fn results.
# Use for eval loops where fn returns a count (e.g. correct predictions).
#   correct = eachBatchCount(images, labels, 100, (imgs, lbls) => {
#       preds = argmax(model(imgs), 1).squeeze(1)
#       countEqual(preds, lbls)
#   })
pub eachBatchCount(data: Tensor, labels: Tensor, batchSize: Int, fn: (Tensor, Tensor) -> Int) -> Int = {
    n = tensorShape(data)[0]
    var total = 0
    var start = 0
    while start < n {
        curBatch = min(batchSize, n - start)
        batchData = data.sliceDim(0, start, curBatch)
        batchLabels = labels.sliceDim(0, start, curBatch)
        total = total + fn(batchData, batchLabels)
        start = start + batchSize
    }
    total
}
