# Load Pretrained BERT and Verify
# This loads google/bert_uncased_L-2_H-128_A-2 model
# Run: nostos --use candle examples/12_bert_pretrained.nos

use candle.*

# =============================================================================
# Helper Functions
# =============================================================================

splitHeads(x, numHeads) = {
    shape = tensorShape(x)
    batch = shape[0]
    seq = shape[1]
    hidden = shape[2]
    headDim = hidden / numHeads
    reshaped = reshape(x, [batch, seq, numHeads, headDim])
    swapDims(reshaped, 1, 2)
}

mergeHeads(x) = {
    shape = tensorShape(x)
    batch = shape[0]
    numHeads = shape[1]
    seq = shape[2]
    headDim = shape[3]
    hidden = numHeads * headDim
    transposed = swapDims(x, 1, 2)
    reshape(transposed, [batch, seq, hidden])
}

multiHeadAttention(q, k, v, numHeads, headDimF, mask) = {
    qHeads = contiguous(splitHeads(q, numHeads))
    kHeads = contiguous(splitHeads(k, numHeads))
    vHeads = contiguous(splitHeads(v, numHeads))
    scale = tensorSqrt(fromList([headDimF]))
    kT = contiguous(swapDims(kHeads, 2, 3))
    scores = matmul(qHeads, kT)
    scaledScores = tensorDiv(scores, scale)
    maskedScores = applyAttentionMask(scaledScores, mask)
    attnWeights = softmax(maskedScores, 3)
    attnOutput = matmul(attnWeights, vHeads)
    mergeHeads(contiguous(attnOutput))
}

bertEmbeddings(tokenIds, tokenTypeIds, wordEmb, posEmb, typeEmb, lnGamma, lnBeta) = {
    tokEmb = embedding(tokenIds, wordEmb)
    seqLen = (tensorShape(tokenIds))[1]
    posIds = unsqueeze(arange(seqLen), 0)
    posEmbedded = embedding(posIds, posEmb)
    typeEmbedded = embedding(tokenTypeIds, typeEmb)
    combined = tensorAdd(tokEmb, posEmbedded)
    combined2 = tensorAdd(combined, typeEmbedded)
    layerNorm(combined2, lnGamma, lnBeta)
}

bertEncoderLayer(x, mask, wQ, wK, wV, wO, bQ, bK, bV, bO, ln1G, ln1B, wFF1, bFF1, wFF2, bFF2, ln2G, ln2B, numHeads, headDimF) = {
    q = linearBias(x, wQ, bQ)
    k = linearBias(x, wK, bK)
    v = linearBias(x, wV, bV)
    attnRaw = multiHeadAttention(q, k, v, numHeads, headDimF, mask)
    attnOut = linearBias(attnRaw, wO, bO)
    hidden1 = layerNorm(tensorAdd(x, attnOut), ln1G, ln1B)

    ffHidden = linearBias(hidden1, wFF1, bFF1)
    ffAct = gelu(ffHidden)
    ffOut = linearBias(ffAct, wFF2, bFF2)
    layerNorm(tensorAdd(hidden1, ffOut), ln2G, ln2B)
}

# =============================================================================
# Main
# =============================================================================

main() = {
    println("=== Loading Pretrained BERT ===")
    println("Model: google/bert_uncased_L-2_H-128_A-2\n")

    # Config
    numHeads = 2
    headDimF = 64.0

    # Load model
    println("Loading safetensors...")
    model = loadSafetensors("/home/user/nostos-candle/models/bert-small.safetensors")
    println("Model loaded!\n")

    # Load embeddings
    println("Loading embedding weights...")
    wordEmb = getTensor(model, "bert.embeddings.word_embeddings.weight")
    posEmb = getTensor(model, "bert.embeddings.position_embeddings.weight")
    typeEmb = getTensor(model, "bert.embeddings.token_type_embeddings.weight")
    embLnG = getTensor(model, "bert.embeddings.LayerNorm.weight")
    embLnB = getTensor(model, "bert.embeddings.LayerNorm.bias")
    println("Word embedding shape:")
    println(tensorShape(wordEmb))

    # Load encoder layer 0
    println("\nLoading layer 0...")
    l0wQ = getTensor(model, "bert.encoder.layer.0.attention.self.query.weight")
    l0wK = getTensor(model, "bert.encoder.layer.0.attention.self.key.weight")
    l0wV = getTensor(model, "bert.encoder.layer.0.attention.self.value.weight")
    l0wO = getTensor(model, "bert.encoder.layer.0.attention.output.dense.weight")
    l0bQ = getTensor(model, "bert.encoder.layer.0.attention.self.query.bias")
    l0bK = getTensor(model, "bert.encoder.layer.0.attention.self.key.bias")
    l0bV = getTensor(model, "bert.encoder.layer.0.attention.self.value.bias")
    l0bO = getTensor(model, "bert.encoder.layer.0.attention.output.dense.bias")
    l0ln1G = getTensor(model, "bert.encoder.layer.0.attention.output.LayerNorm.weight")
    l0ln1B = getTensor(model, "bert.encoder.layer.0.attention.output.LayerNorm.bias")
    l0wFF1 = getTensor(model, "bert.encoder.layer.0.intermediate.dense.weight")
    l0bFF1 = getTensor(model, "bert.encoder.layer.0.intermediate.dense.bias")
    l0wFF2 = getTensor(model, "bert.encoder.layer.0.output.dense.weight")
    l0bFF2 = getTensor(model, "bert.encoder.layer.0.output.dense.bias")
    l0ln2G = getTensor(model, "bert.encoder.layer.0.output.LayerNorm.weight")
    l0ln2B = getTensor(model, "bert.encoder.layer.0.output.LayerNorm.bias")

    # Load encoder layer 1
    println("Loading layer 1...")
    l1wQ = getTensor(model, "bert.encoder.layer.1.attention.self.query.weight")
    l1wK = getTensor(model, "bert.encoder.layer.1.attention.self.key.weight")
    l1wV = getTensor(model, "bert.encoder.layer.1.attention.self.value.weight")
    l1wO = getTensor(model, "bert.encoder.layer.1.attention.output.dense.weight")
    l1bQ = getTensor(model, "bert.encoder.layer.1.attention.self.query.bias")
    l1bK = getTensor(model, "bert.encoder.layer.1.attention.self.key.bias")
    l1bV = getTensor(model, "bert.encoder.layer.1.attention.self.value.bias")
    l1bO = getTensor(model, "bert.encoder.layer.1.attention.output.dense.bias")
    l1ln1G = getTensor(model, "bert.encoder.layer.1.attention.output.LayerNorm.weight")
    l1ln1B = getTensor(model, "bert.encoder.layer.1.attention.output.LayerNorm.bias")
    l1wFF1 = getTensor(model, "bert.encoder.layer.1.intermediate.dense.weight")
    l1bFF1 = getTensor(model, "bert.encoder.layer.1.intermediate.dense.bias")
    l1wFF2 = getTensor(model, "bert.encoder.layer.1.output.dense.weight")
    l1bFF2 = getTensor(model, "bert.encoder.layer.1.output.dense.bias")
    l1ln2G = getTensor(model, "bert.encoder.layer.1.output.LayerNorm.weight")
    l1ln2B = getTensor(model, "bert.encoder.layer.1.output.LayerNorm.bias")

    # Load pooler
    poolerW = getTensor(model, "bert.pooler.dense.weight")
    poolerB = getTensor(model, "bert.pooler.dense.bias")
    println("Weights loaded!\n")

    # Test input: "hello world" -> [CLS]=101, hello=7592, world=2088, [SEP]=102
    println("Input: \"hello world\"")
    println("Token IDs: [101, 7592, 2088, 102]")

    # Create tensors
    tokenTensor = fromList([[101, 7592, 2088, 102]])
    println("Token tensor shape:")
    println(tensorShape(tokenTensor))

    # Token types (all zeros for single sentence)
    typeIds = fromList([[0, 0, 0, 0]])

    # Create mask (no padding for this input)
    mask = createAttentionMask(tokenTensor, 0)

    # Forward pass
    println("\n--- Forward Pass ---\n")

    println("1. Embeddings")
    embedded = bertEmbeddings(tokenTensor, typeIds, wordEmb, posEmb, typeEmb, embLnG, embLnB)
    println("   Shape:")
    println(tensorShape(embedded))

    embSlice = narrow(narrow(embedded, 0, 0, 1), 1, 0, 1)
    embValues = squeeze(squeeze(embSlice, 0), 0)
    println("   First 5 values of CLS embedding:")
    println(toList(narrow(embValues, 0, 0, 5)))

    println("\n2. Encoder Layer 0")
    hidden0 = bertEncoderLayer(embedded, mask, l0wQ, l0wK, l0wV, l0wO, l0bQ, l0bK, l0bV, l0bO, l0ln1G, l0ln1B, l0wFF1, l0bFF1, l0wFF2, l0bFF2, l0ln2G, l0ln2B, numHeads, headDimF)
    println("   Shape:")
    println(tensorShape(hidden0))

    println("\n3. Encoder Layer 1")
    hidden1 = bertEncoderLayer(hidden0, mask, l1wQ, l1wK, l1wV, l1wO, l1bQ, l1bK, l1bV, l1bO, l1ln1G, l1ln1B, l1wFF1, l1bFF1, l1wFF2, l1bFF2, l1ln2G, l1ln2B, numHeads, headDimF)
    println("   Shape:")
    println(tensorShape(hidden1))

    # Pooler
    println("\n4. Pooler (CLS token)")
    clsToken = narrow(hidden1, 1, 0, 1)
    clsFlat = squeeze(clsToken, 1)
    pooled = tanh(linearBias(clsFlat, poolerW, poolerB))
    println("   Pooled shape:")
    println(tensorShape(pooled))

    println("\n   First 10 pooled values:")
    pooledFirst = narrow(squeeze(pooled, 0), 0, 0, 10)
    println(toList(pooledFirst))

    println("\n=== Pretrained BERT Complete ===")

    0
}
