# ModernBERT Model - Clean Implementation
# Demonstrates: string interpolation, fold, clean model interface
# Run: nostos --use candle examples/15_models.nos

use candle.*

# =============================================================================
# Tokenization
# =============================================================================

getTokenIds(text) = {
    tok = loadTokenizer("/home/user/nostos-candle/models/modernbert-tokenizer.json")
    encode(tok, text)
}

# =============================================================================
# Attention Helpers
# =============================================================================

splitHeads(x, numHeads) = {
    shape = tensorShape(x)
    batch = shape[0]
    seq = shape[1]
    hidden = shape[2]
    headDim = hidden / numHeads
    reshaped = reshape(x, [batch, seq, numHeads, headDim])
    swapDims(reshaped, 1, 2)
}

mergeHeads(x) = {
    shape = tensorShape(x)
    batch = shape[0]
    numHeads = shape[1]
    seq = shape[2]
    headDim = shape[3]
    hidden = numHeads * headDim
    transposed = swapDims(x, 1, 2)
    reshape(transposed, [batch, seq, hidden])
}

applyRopeToQK(x, cos, sin) = {
    shape = tensorShape(x)
    headDim = shape[3]
    halfDim = headDim / 2
    x1 = narrow(x, 3, 0, halfDim)
    x2 = narrow(x, 3, halfDim, halfDim)
    cosHalf = narrow(cos, 1, 0, halfDim)
    sinHalf = narrow(sin, 1, 0, halfDim)
    cosB = unsqueeze(unsqueeze(cosHalf, 0), 0)
    sinB = unsqueeze(unsqueeze(sinHalf, 0), 0)
    rotX1 = tensorSub(tensorMul(x1, cosB), tensorMul(x2, sinB))
    rotX2 = tensorAdd(tensorMul(x1, sinB), tensorMul(x2, cosB))
    cat([rotX1, rotX2], 3)
}

# =============================================================================
# ModernBERT Components
# =============================================================================

modernBertAttention(x, wQKV, wO, cos, sin, numHeads, headDimF) = {
    qkv = linear(x, wQKV)
    shape = tensorShape(qkv)
    hidden = shape[2] / 3
    q = narrow(qkv, 2, 0, hidden)
    k = narrow(qkv, 2, hidden, hidden)
    v = narrow(qkv, 2, hidden * 2, hidden)
    qHeads = contiguous(splitHeads(q, numHeads))
    kHeads = contiguous(splitHeads(k, numHeads))
    vHeads = contiguous(splitHeads(v, numHeads))
    qRope = applyRopeToQK(qHeads, cos, sin)
    kRope = applyRopeToQK(kHeads, cos, sin)
    scale = tensorSqrt(fromList([headDimF]))
    kT = contiguous(swapDims(kRope, 2, 3))
    scores = matmul(qRope, kT)
    scaledScores = tensorDiv(scores, scale)
    attnWeights = softmax(scaledScores, 3)
    attnOutput = matmul(attnWeights, vHeads)
    merged = mergeHeads(contiguous(attnOutput))
    linear(merged, wO)
}

modernBertMLP(x, wI, wO) = {
    gateUp = linear(x, wI)
    shape = tensorShape(gateUp)
    intermediate = shape[2] / 2
    gate = narrow(gateUp, 2, 0, intermediate)
    up = narrow(gateUp, 2, intermediate, intermediate)
    hidden = geglu(gate, up)
    linear(hidden, wO)
}

# Layer 0 has no attn_norm (embeddings are already normalized)
modernBertLayer0(x, wQKV, wO, mlpNormW, wI, wO2, cos, sin, numHeads, headDimF, hidden) = {
    attnOut = modernBertAttention(x, wQKV, wO, cos, sin, numHeads, headDimF)
    hidden1 = tensorAdd(x, attnOut)
    zerosBias = zeros([hidden])
    mlpIn = layerNorm(hidden1, mlpNormW, zerosBias)
    mlpOut = modernBertMLP(mlpIn, wI, wO2)
    tensorAdd(hidden1, mlpOut)
}

# Layers 1-21 have attn_norm
modernBertLayerN(x, attnNormW, wQKV, wO, mlpNormW, wI, wO2, cos, sin, numHeads, headDimF, hidden) = {
    zerosBias = zeros([hidden])
    attnIn = layerNorm(x, attnNormW, zerosBias)
    attnOut = modernBertAttention(attnIn, wQKV, wO, cos, sin, numHeads, headDimF)
    hidden1 = tensorAdd(x, attnOut)
    mlpIn = layerNorm(hidden1, mlpNormW, zerosBias)
    mlpOut = modernBertMLP(mlpIn, wI, wO2)
    tensorAdd(hidden1, mlpOut)
}

# =============================================================================
# Generic Weight Loading (using string interpolation)
# =============================================================================

# Load weights for layer 0 (no attn_norm)
loadLayer0Weights(model) = {
    [
        getTensor(model, "model.layers.0.attn.Wqkv.weight"),
        getTensor(model, "model.layers.0.attn.Wo.weight"),
        getTensor(model, "model.layers.0.mlp_norm.weight"),
        getTensor(model, "model.layers.0.mlp.Wi.weight"),
        getTensor(model, "model.layers.0.mlp.Wo.weight")
    ]
}

# Load weights for layers 1-21 (with attn_norm)
loadLayerNWeights(model, i) = {
    [
        getTensor(model, "model.layers.${i}.attn_norm.weight"),
        getTensor(model, "model.layers.${i}.attn.Wqkv.weight"),
        getTensor(model, "model.layers.${i}.attn.Wo.weight"),
        getTensor(model, "model.layers.${i}.mlp_norm.weight"),
        getTensor(model, "model.layers.${i}.mlp.Wi.weight"),
        getTensor(model, "model.layers.${i}.mlp.Wo.weight")
    ]
}

# =============================================================================
# Helper Functions
# =============================================================================

printFirst5CLS(x, suffix) = {
    slice = narrow(narrow(x, 0, 0, 1), 1, 0, 1)
    first = squeeze(squeeze(slice, 0), 0)
    println("   First 5 values of CLS" ++ suffix ++ ":")
    println(toList(narrow(first, 0, 0, 5)))
}

# Check if layer uses global attention (every 3rd: 0, 3, 6, 9, 12, 15, 18, 21)
isGlobalLayer(i) = i % 3 == 0

# =============================================================================
# Forward Pass with Fold
# =============================================================================

runEmbeddings(tokenTensor, model) = {
    tokEmb = getTensor(model, "model.embeddings.tok_embeddings.weight")
    embNormW = getTensor(model, "model.embeddings.norm.weight")
    embedded = embedding(tokenTensor, tokEmb)
    println("1. Token Embeddings - Shape:")
    println(tensorShape(embedded))
    printFirst5CLS(embedded, "   (before norm)")
    zerosBias = zeros([768])
    embNormed = layerNorm(embedded, embNormW, zerosBias)
    println("\n2. After Embedding Norm")
    printFirst5CLS(embNormed, "")
    embNormed
}

# Run a single layer (generic for any layer index)
runSingleLayer(x, model, i, cosGlobal, sinGlobal, cosLocal, sinLocal) = {
    isGlobal = isGlobalLayer(i)
    cos = if isGlobal then cosGlobal else cosLocal
    sin = if isGlobal then sinGlobal else sinLocal
    attnType = if isGlobal then "global" else "local"

    result = if i == 0 then {
        weights = loadLayer0Weights(model)
        modernBertLayer0(x, weights[0], weights[1], weights[2], weights[3], weights[4], cos, sin, 12, 64.0, 768)
    } else {
        weights = loadLayerNWeights(model, i)
        modernBertLayerN(x, weights[0], weights[1], weights[2], weights[3], weights[4], weights[5], cos, sin, 12, 64.0, 768)
    }

    stepNum = i + 3
    println("\n${stepNum}. After Layer ${i} (${attnType})")
    printFirst5CLS(result, "")
    result
}

# Run all 22 layers using fold over a list of indices
runAllLayersFold(x, model, cosGlobal, sinGlobal, cosLocal, sinLocal) = {
    # Create list of layer indices [0, 1, 2, ..., 21]
    indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]

    # Fold over indices, threading the hidden state through
    indices.fold(x, (hidden, i) => runSingleLayer(hidden, model, i, cosGlobal, sinGlobal, cosLocal, sinLocal))
}

# =============================================================================
# Main
# =============================================================================

main() = {
    println("=== ModernBERT Refactored (22 Layers with Fold) ===")
    println("Model: answerdotai/ModernBERT-base\n")

    # Tokenize
    testText = "hello world"
    println("Input: \"" ++ testText ++ "\"")
    tokenIds = getTokenIds(testText)
    println("Token IDs:")
    println(tokenIds)

    # Convert to tensor
    tokenTensor = unsqueeze(fromIntList(tokenIds), 0)
    println("Token tensor shape:")
    println(tensorShape(tokenTensor))
    seqLen = (tensorShape(tokenTensor))[1]

    # Load model
    println("\nLoading safetensors...")
    model = loadSafetensors("/home/user/nostos-candle/models/modernbert-base.safetensors")
    println("Model loaded!")

    # Compute RoPE frequencies
    println("Computing RoPE frequencies...")
    ropeGlobal = ropeFreqs(seqLen, 64, 160000.0)
    ropeLocal = ropeFreqs(seqLen, 64, 10000.0)
    println("RoPE computed!\n")

    # Forward pass
    println("--- Forward Pass (All 22 Layers) ---\n")

    # Embeddings
    embNormed = runEmbeddings(tokenTensor, model)

    # All 22 layers using fold
    finalOutput = runAllLayersFold(embNormed, model, ropeGlobal[0], ropeGlobal[1], ropeLocal[0], ropeLocal[1])

    println("\n=== ModernBERT Refactored Complete ===")
    println("Successfully ran all 22 layers with fold!")

    0
}
