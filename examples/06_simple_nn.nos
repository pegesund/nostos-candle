# Simple Neural Network Forward Pass (No Bias)
# Run: nostos --use candle examples/06_simple_nn.nos

use candle.*

main() = {
    println("=== Simple Neural Network Forward Pass ===\n")
    println("Network: Input(2) -> Hidden(3) -> Output(2)")
    println("Activation: ReLU for hidden, Softmax for output\n")

    # Input: batch of 2 samples, 2 features each
    x = fromList([[1.0, 0.5],
                  [0.2, 0.8]])
    println("Input (batch_size=2, features=2):")
    println(toList(x))

    # Layer 1: Linear (2 -> 3)
    w1 = fromList([[0.1, 0.2, 0.3],
                   [0.4, 0.5, 0.6]])  # 2x3

    println("\n--- Layer 1 (Linear + ReLU) ---")
    println("W1 shape:")
    println(tensorShape(w1))

    # Forward: x @ w1
    h1_pre = matmul(x, w1)
    println("Pre-activation (x @ w1):")
    println(toList(h1_pre))

    h1 = relu(h1_pre)
    println("After ReLU:")
    println(toList(h1))

    # Layer 2: Linear (3 -> 2)
    w2 = fromList([[0.2, 0.3],
                   [0.4, 0.5],
                   [0.6, 0.7]])  # 3x2

    println("\n--- Layer 2 (Linear + Softmax) ---")
    println("W2 shape:")
    println(tensorShape(w2))

    # Forward: h1 @ w2
    logits = matmul(h1, w2)
    println("Logits (h1 @ w2):")
    println(toList(logits))

    # Softmax for probabilities
    probs = softmax(logits, 1)
    println("Output probabilities:")
    println(toList(probs))

    println("\n--- Result ---")
    println("Each row sums to 1 (probability distribution)")

    0
}
