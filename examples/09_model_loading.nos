# Model Loading from Safetensors
# Demonstrates how to load pre-trained models
# Run: nostos --use candle examples/09_model_loading.nos
#
# Note: You need a .safetensors file to run this example.
# Download a model from HuggingFace, e.g.:
#   wget https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/model.safetensors

use candle.*

main() = {
    println("=== Model Loading from Safetensors ===\n")

    # Example path - replace with your actual model file
    modelPath = "model.safetensors"

    println("This example shows how to load a pre-trained model.")
    println("To run it, download a safetensors file first.\n")

    println("API Overview:")
    println("--------------")
    println("1. loadSafetensors(path) - Load model weights")
    println("2. listTensors(model) - List all tensor names")
    println("3. getTensor(model, name) - Get specific tensor")
    println("4. tensorShape(t) - Check tensor dimensions")
    println("5. dtype(t) - Check tensor data type\n")

    println("Example usage (pseudo-code):")
    println("-----------------------------")
    println("model = loadSafetensors(\"model.safetensors\")")
    println("names = listTensors(model)")
    println("println(names)  # See all available weights")
    println("")
    println("# Get specific weights")
    println("embeddings = getTensor(model, \"embeddings.word_embeddings.weight\")")
    println("println(tensorShape(embeddings))  # e.g., [30522, 384]")
    println("")
    println("# Use weights in forward pass")
    println("tokenIds = fromList([[101, 2054, 2003, 1029, 102]])")
    println("embedded = embedding(tokenIds, embeddings)")
    println("")

    # Demonstrate creating a simple model structure
    println("\n=== Building a Mini Model ===\n")

    vocabSize = 1000
    hiddenSize = 64
    numClasses = 3

    # Create "model weights"
    embWeight = randn([vocabSize, hiddenSize])
    layerWeight = randn([hiddenSize, hiddenSize])
    layerBias = zeros([hiddenSize])
    layerNormGamma = ones([hiddenSize])
    layerNormBeta = zeros([hiddenSize])
    classifierWeight = randn([numClasses, hiddenSize])
    classifierBias = zeros([numClasses])

    println("Model architecture:")
    println("  Embedding: [1000, 64]")
    println("  Layer: [64, 64] + bias")
    println("  LayerNorm: [64]")
    println("  Classifier: [3, 64] + bias")

    # Forward pass
    println("\nForward pass with dummy input:")
    tokenIds = fromList([[5, 10, 15, 20, 1]])  # 1 sample, 5 tokens
    println("Input token IDs:")
    println(toList(tokenIds))

    # Embedding lookup
    embedded = embedding(tokenIds, embWeight)
    println("After embedding:")
    println(tensorShape(embedded))

    # Linear + LayerNorm
    hidden = linearBias(embedded, layerWeight, layerBias)
    normalized = layerNorm(hidden, layerNormGamma, layerNormBeta)
    println("After linear + layernorm:")
    println(tensorShape(normalized))

    # Mean pooling over sequence
    meanPooled = tensorMeanDim(normalized, 1)
    pooled = squeeze(meanPooled, 1)
    println("After mean pooling:")
    println(tensorShape(pooled))

    # Classification
    logits = linearBias(pooled, classifierWeight, classifierBias)
    probs = softmax(logits, 1)
    println("Classification probabilities:")
    println(toList(probs))

    # Get prediction
    pred = argmax(probs, 1)
    println("Predicted class:")
    println(toList(pred))

    0
}
